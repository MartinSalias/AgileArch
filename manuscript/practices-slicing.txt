
{pagebreak}

## Slicing

The concept of slicing or breaking down User Stories (although the general idea can be applied to any other unit of work you use) into smaller ones is one of the most important aspects of Agile Software Development.

As a community, we agree that the smaller (but valuable) a User Story is, the better, for several reasons:

- More understandable
- Simpler to prioritize (or re-prioritize)
- Easier to estimate
- Easier to decompose in implementation tasks
- Fewer acceptance criteria; easier to test

Above anything else, when we break down a User Story into several smaller ones, we have more chances to find less valuable items that can be deferred for later, and eventually discarded if they are not valuable enough as the development goes on. This is exactly the Agile Principle of Simplicity, defined as "the art of maximizing the amount
of work not done".

W> ## About "Technical" User Stories
W>
W> Before moving forward I want to highlight that I don't believe in "Technical" User Stories.
W>
W> My point is that a User Story should be valuable to the end user, not to the team, which is typically the case for "technical" items.
W>
W> But to properly solve User Stories, the dev team needs to break it down in Implementation Tasks. Most of the time, architectural issues appear at that level.
W>
W> Even if you are building Development Tools and Frameworks (something I did many times), then all your User Stories might be technical in nature, but they need to cover real user needs.

{pagebreak}

### Functional Splitting Patterns

One of my favorite resources to help teams to slice (or split) User Stories is Richard Lawrence's guide "Patterns for Splitting User Stories" [^PSUS].

The guide proposes a three-step approach:

1. Check the User Story quality and fix other problems before splitting
2. Try a series of patterns to find ways to split it
3. Evaluate the resulting User Stories to be sure everything is fine

What's most important for us now are the patterns themselves. I recommend you to read the original post and even find out some additional patterns based on your experience, but let's list the 9 original patterns before extending them.

1. Workflow Steps
2. Business Rule Variation
3. Major Effort
4. Simple / Complex
5. Variations in Data
6. Data Entry Methods
7. Defer Performance
8. Operations
9. Break Out a Spike

Even without getting into details you can see the focus is to try to break down functionality in more granular parts, trying to be able to take the most valuable and ideally thinner part of each need, and provide the earliest and more valuable implementation, trying to prove or discard the hypothesis behind each User Story and also leveraging all the learning from this early delivery to better select what's next, the best approach to implement it, and even what's not needed at all.

### Architectural Splitting Patterns

Once you broke down your User Stories into smaller ones, you can still apply slicing strategies based on architectural concerns.

You may have smelled some architectural patterns in the list above. For example: Variations in Data can be considered also as "Variations in protocols, message format, encoding, and other messaging attributes". Also, "Defer Performance", which tries to solve first the functional aspects of a User Story and leave a __potential__ optimization as a later Story, can be applied to several other Quality Attributes.

Then, you can apply further splitting patterns when breaking each User Story down to its  Tasks. What's important is to reflect in the resulting User Stories this restriction, and keep being explicit about the remaining value.

Some of the extended Splitting Patterns I found useful from experience are:

#### User Experience level

This approach is actually a huge topic in itself, but basically focus in providing the simpler UX approach first, iteratively test it with real users and keep refining for as long as needed, depending on the product.

The UX field is so large and important right now that I prefer to point you elsewhere, probably starting with the books "Lean UX", by Jeff Gothelf and Josh Seiden, and "User Story Mapping", by Jeff Patton.

#### Reliability Constraints

User Stories related to reliability (when properly analyzed) tend to have strong acceptance criteria about accurate data input and data transformations, error-free state management, and non-corrupting recovery from detected failure condition.

This is not typically the kind of User Stories you get at first, but they appear through Backlog Refinement and usually come from observed behavior after the application is already in use (or under specific testing conditions).

The different constraints related to this normally have a highly different business value and implementation costs, so separating each constraint base on its calculated Return of Investment can be a smart move, and just doing this analysis can provide many insights for both the involved stakeholders and development team members.

#### Manageability / Security levels

Configuration aspects can be split over time, deferring complexity from (hopefully short-term) hard-coded values, config files/resources, admin dashboards, cold or hot reconfiguration mechanics, and more.

In a similar way, Security can be split along different axis like internal/external, authentication/authorization, user-level, api-level, component-level, code-access-level, resource-level (like DB and services accounts) and much more. Security is always a complex and sometimes very restrictive, so the ability to spread various security concerns over a long period tends to be a good idea. In very sensitive scenarios, each User Story can have very specific Security Acceptance Criteria, or can be checked against a specific set of Security concerns gradually added to the Definition of Done.

#### Defer Performance / Availability / Efficiency / Scalability

As important as these factors can be in different scenarios, they are usually pretty difficult to plan ahead without over-engineering. As we will see in other sections, a recurring pattern is to agree on some baseline value as part of the User Story Acceptance Criteria and build an automated test to verify this. Of course such tests are not always trivial, and that encourage good conversation with stakeholders about the real importance of these attributes in a given context.

It should be also noted that building the first few testing scenarios for these Quality Attributes take more effort, but the required environments and tools and scripts used tend to stabilize over time and further automation effort usually decrease.

#### Staged Portability / Interoperability / Internationalization

For every User Story dealing with these attributes tend to be breath goals like:

- target platforms
- systems, protocols, messaging, data sources
- languages or locales to support

But each of these dimensions have also depth goals:

- versions (or version ranges) within each platform
- versions, encodings, message formats
- static text, dynamic text, localized media, date and currency formats, calendars

In many cases there can be a complex matrix for each of this Quality Attribute that can be split over time. Staging this changes usually break down functional and technical implementation.

#### Taking / Paying Technical Debt

I like to take this as a separate pattern that can encompass any of the previous attributes, and it uses the Technical Debt metaphor coined by Ward Cunningham[^TD].

Following the initial idea, when the team takes Technical Debt, it should be explicit and managed. This is very different than implementing "just what is needed". If you write clean code without extensibility points that you don't need yet, this is not Debt at all. You will just have to spend a bit more refactoring whenever (and if) the extensibility is needed later on. This kind of decision is not incurring on any interest.

Now, if the team is explicitly leaving a substandard or partial implementation of a component (one with known risks and issues) then they have to track this, and my favorite way to do it is by thinking what is the problem/risk it generates for the users, discussing it with the stakeholders, and then coming up with a User Story expressing that need: not the technical detail, but the need to prevent the unwanted circumstance the current implementation leaves open.


[^PSUS]: http://agileforall.com/patterns-for-splitting-user-stories/
[^TD]: Explained at Martin Fowler's bliki: https://martinfowler.com/bliki/TechnicalDebt.html
